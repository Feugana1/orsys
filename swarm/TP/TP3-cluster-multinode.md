# TP3: Swarm Cluster Multi-N≈ìuds

**Niveau:** ‚≠ê‚≠ê‚≠ê Avanc√©
**Dur√©e:** 1.5 heures
**Objectif:** Cr√©er un cluster Swarm avec 3 n≈ìuds (VMs ou Docker Desktop)

---

## üìö Concepts Couverts

- Manager vs Worker roles
- Token de jointure
- Node labels et constraints
- Quorum et consensus Raft
- Health checks

---

## üéØ Exercice 1: Pr√©parer 3 N≈ìuds

### Option A: Avec Docker Desktop + Docker in Docker

```bash
# Cr√©er 3 conteneurs pour simuler 3 n≈ìuds
docker run -d --name node1 --privileged -it docker:dind
docker run -d --name node2 --privileged -it docker:dind
docker run -d --name node3 --privileged -it docker:dind

# V√©rifier
docker ps | grep node
```

### Option B: Avec Docker en r√©seau

```bash
# Cr√©er un network
docker network create --driver bridge swarm-net

# Lancer 3 conteneurs connect√©s
docker run -d --name node1 --network swarm-net --privileged docker:dind
docker run -d --name node2 --network swarm-net --privileged docker:dind
docker run -d --name node3 --network swarm-net --privileged docker:dind
```

### Option C: Avec 3 VMs (VirtualBox, Hyper-V)

```bash
# Cr√©er 3 VMs avec Docker install√©
# Ubuntu 20.04 + Docker Engine
# node1: 192.168.0.10
# node2: 192.168.0.11
# node3: 192.168.0.12
```

---

## üöÄ Exercice 2: Initialiser le Cluster

### √âtape 1: Initialiser le manager (node1)

```bash
# Sur node1
docker swarm init --advertise-addr <IP_node1>

# R√©sultat:
# Swarm initialized: current node (xyz123...) is now a manager.
```

### √âtape 2: R√©cup√©rer les tokens

```bash
# Token worker (sur node1)
docker swarm join-token worker
# SWMTKN-1-5abc...

# Token manager (sur node1)
docker swarm join-token manager
# SWMTKN-1-4xyz...
```

### √âtape 3: Ajouter les workers

```bash
# Sur node2
docker swarm join --token SWMTKN-1-5abc... <IP_node1>:2377

# Sur node3
docker swarm join --token SWMTKN-1-5abc... <IP_node1>:2377

# R√©sultat sur chaque:
# This node joined a swarm as a worker.
```

### √âtape 4: V√©rifier le cluster

```bash
# Sur node1 (manager)
docker node ls

# R√©sultat:
# ID          HOSTNAME    STATUS    AVAILABILITY    MANAGER STATUS
# xyz123*     node1       Ready     Active           Leader
# abc456      node2       Ready     Active
# def789      node3       Ready     Active
```

---

## üè∑Ô∏è Exercice 3: Lab√©liser les N≈ìuds

### √âtape 1: Ajouter des labels

```bash
# Sur node1 (manager)
docker node update --label-add role=web node2
docker node update --label-add role=db node3
docker node update --label-add role=manager node1

# V√©rifier
docker node inspect node2 --pretty | grep -A 5 "Labels"
```

### √âtape 2: Utiliser les labels pour le placement

```bash
# Cr√©er une stack avec constraints
cat > stack-multinode.yml << 'EOF'
version: '3.8'

services:
  frontend:
    image: nginx:latest
    ports:
      - "8080:80"
    deploy:
      replicas: 2
      placement:
        constraints: [node.labels.role == web]
    networks:
      - webnet

  database:
    image: postgres:14
    environment:
      POSTGRES_PASSWORD: password123
    deploy:
      replicas: 1
      placement:
        constraints: [node.labels.role == db]
    volumes:
      - db_data:/var/lib/postgresql/data
    networks:
      - webnet

networks:
  webnet:

volumes:
  db_data:
EOF

# D√©ployer
docker stack deploy -c stack-multinode.yml mystack

# V√©rifier le placement
docker stack ps mystack
# frontend doit √™tre sur node2
# database doit √™tre sur node3
```

---

## üëî Exercice 4: Managers et Workers

### √âtape 1: Promouvoir un worker en manager

```bash
# Sur node1 (manager)
docker node promote node2

# V√©rifier
docker node ls
# node2 doit avoir MANAGER STATUS = "Reachable"
```

### √âtape 2: Observer le quorum Raft

```bash
# Voir l'√©tat du cluster
docker info | grep -A 20 "Swarm"

# Nombre de managers
docker node ls | grep -i "manager\|leader" | wc -l
```

### √âtape 3: R√©trograder un manager

```bash
# Sur node1 (manager)
docker node demote node2

# V√©rifier
docker node ls
# node2 doit revenir √† MANAGER STATUS vide
```

---

## üåê Exercice 5: Communication Inter-N≈ìuds

### √âtape 1: Voir le networking

```bash
# Sur node1
docker network ls | grep webnet

# Voir le network Swarm
docker network inspect webnet_webnet

# Tous les n≈ìuds doivent voir les services
docker service ps mystack
# Services r√©partis sur node1, node2, node3
```

### √âtape 2: Test de connectivit√©

```bash
# Sur node1, acc√©der √† un service sur node3
curl http://localhost:8080

# Les requests sont distribu√©es
for i in {1..10}; do
  curl -s http://localhost:8080 | grep "Server:" | head -1
done
```

---

## üìä Exercice 6: Monitoring et Logs

### √âtape 1: Logs distribu√©es

```bash
# Voir les logs du service depuis n'importe quel n≈ìud
docker service logs mystack_database

# Logs d'un conteneur sp√©cifique
docker service ps mystack_frontend
docker logs <container_id>
```

### √âtape 2: Health check

```bash
# Les services sont monitored automatiquement
docker service ps mystack

# Voir l'√©tat de chaque r√©plica
# Si un conteneur crash, Swarm le relance
```

---

## üõ†Ô∏è Exercice 7: Mises √† Jour en Cluster

### √âtape 1: Rolling update

```bash
# Mettre √† jour l'image
docker service update \
  --image nginx:alpine \
  mystack_frontend

# Observer les mises √† jour
docker service ps mystack_frontend --no-trunc

# Les updates progressent: node √† node
```

### √âtape 2: Contr√¥ler la vitesse

```bash
# Mise √† jour lente (1 √† la fois, d√©lai 30s)
docker service update \
  --update-parallelism 1 \
  --update-delay 30s \
  --image nginx:1.25 \
  mystack_frontend
```

---

## ‚ö†Ô∏è Exercice 8: R√©silience - Simuler une Panne

### √âtape 1: Arr√™ter un worker

```bash
# Sur node2, arr√™ter Docker
docker stop <container_if_dind>  # Si utilisant dind

# Ou sur VM: systemctl stop docker
```

### √âtape 2: Observer la redistribution

```bash
# Sur node1 (manager)
docker service ps mystack

# Les conteneurs de node2 devraient √™tre relanc√©s sur node3
# Le cluster se r√©√©quilibre automatiquement
```

### √âtape 3: Red√©marrer le n≈ìud

```bash
# Red√©marrer docker sur node2
# Les conteneurs qui avaient crash√© peuvent √™tre red√©ploy√©s

# V√©rifier le status
docker node ls
# node2 revient √† "Ready"
```

---

## ‚úÖ Validation - Checklist

- [ ] 3 n≈ìuds cr√©√©s et connect√©s en r√©seau
- [ ] Cluster Swarm initialis√©
- [ ] `docker node ls` montre 3 n≈ìuds
- [ ] Labels appliqu√©s correctement
- [ ] Stack d√©ploy√©e avec constraints
- [ ] Services plac√©s sur les bons n≈ìuds
- [ ] Manager promotion/demotion fonctionne
- [ ] Quorum visible: `docker info`
- [ ] Logs accessibles de n'importe quel n≈ìud
- [ ] Rolling updates sans downtime

---

## üéì Points Cl√©s √† Retenir

1. **Architecture Multi-N≈ìud**
   - Managers: D√©cisionnaires (min 1)
   - Workers: Ex√©cutants
   - Quorum Raft: Consensus automatique

2. **Node Labels**
   - Placement des services
   - Constraints flexibles
   - Infrastructure as Code

3. **R√©silience Automatique**
   - D√©tection de pannes
   - R√©√©quilibrage automatique
   - Services relanc√©s ailleurs

4. **Networking Overlay**
   - Services communiquent partout
   - Load balancing transparent
   - DNS fonctionnement sur tous les n≈ìuds

5. **Rolling Updates**
   - Z√©ro downtime
   - Contr√¥le de la vitesse
   - Version pr√©c√©dente peut rollback

---

## üîó Prochaine √âtape

‚Üí **TP4-7: Cas Avanc√©s** - HA, Rollback, Production patterns (voir swarm/swarm-team.md)

## üí° Cluster Management

```bash
# Nodes
docker node ls
docker node inspect node1
docker node update --label-add key=value node1
docker node promote node2
docker node demote node2

# Swarm
docker swarm init
docker swarm join-token worker
docker swarm join-token manager
docker swarm leave --force

# Services clustering
docker service create --placement-pref role=manager ...
docker service update --image new:version service-name
```

---

**Fin TP3** ‚úÖ

**Prochaines √©tapes dans swarm-team.md:**
- TP4: Mises √† jour et rollback
- TP5: Haute disponibilit√©
- TP6: Supervision (Portainer)
- TP7: Projet final 3-tier
